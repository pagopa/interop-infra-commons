global:
  logging:
    level: info
    format: text
  env:
      # Description: Limits the number of Kubernetes API requests per second.
      # Default Value: 50
      # Higher values reduce bottlenecks in syncing large applications but can strain the Kubernetes API server. Monitor server metrics to ensure it can handle the increased load.
    - name: ARGOCD_K8S_CLIENT_QPS
      value: "100"
    
      # Description: Defines the maximum burst of requests sent to the Kubernetes API.
      # Default Value: 100
      # Increasing the burst limit improves sync speeds during spikes but could lead to API server throttling if it exceeds capacity.
    - name: ARGOCD_K8S_CLIENT_BURST
      value: "200"
redis-ha:
  enabled: false
redis:
  metrics:
    enabled: false
    service:
      annotations: {}
      clusterIP: None
      labels: {}
      portName: http-metrics
      servicePort: 9121
      type: ClusterIP
    serviceMonitor:
      additionalLabels: {}
      annotations: {}
      enabled: false
      honorLabels: false
      interval: 30s
      metricRelabelings: []
      namespace: ""
      relabelings: []
      scheme: ""
      selector: {}
      tlsConfig: {}  
  resources:
    limits:
      cpu: 200m
      memory: 256Mi
    requests:
      cpu: 10m
      memory: 64Mi
applicationSet:
  metrics:
    enabled: true
    service:
      annotations: {}
      clusterIP: ""
      labels: {}
      portName: http-metrics
      servicePort: 8080
      type: ClusterIP
    serviceMonitor:
      additionalLabels: {}
      annotations: {}
      enabled: true
      honorLabels: false
      interval: 30s
      metricRelabelings: []
      namespace: ""
      relabelings: []
      scheme: ""
      scrapeTimeout: ""
      selector: {}
      tlsConfig: {}
  name: applicationset-controller
  replicas: 1
  resources:
    limits:
      cpu: 150m
      memory: 256Mi
    requests:
      cpu: 80m
      memory: 64Mi
  service:
    annotations: {}
    labels: {}
    port: 7000
    portName: http-webhook
    type: ClusterIP
controller:
  metrics:
    enabled: true
  replicas: 1
  resources:
    limits:
      cpu: 2
      memory: 4Gi
    requests:
      cpu: 2
      memory: 4Gi
  revisionHistoryLimit: 5
repoServer:
  autoscaling:
    enabled: false
    maxReplicas: 5
    minReplicas: 1
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 50
  volumes:
    - name: plugin-home
      emptyDir: {}
    - name: cmp-tmp
      emptyDir: {}
  extraArgs: []
  extraContainers: []
  metrics:
    enabled: false
    service:
      annotations: {}
      clusterIP: ""
      labels: {}
      portName: http-metrics
      servicePort: 8084
      type: ClusterIP
    serviceMonitor:
      additionalLabels: {}
      annotations: {}
      enabled: true
      honorLabels: false
      interval: 30s
      metricRelabelings: []
      namespace: ""
      relabelings: []
      scheme: ""
      scrapeTimeout: ""
      selector: {}
      tlsConfig: {}
  replicas: 2
  resources:
    limits:
      cpu: 1
      memory: 4Gi
    requests:
      cpu: 1
      memory: 4Gi
server:
  replicas: 2
  autoscaling:
    enabled: false
    maxReplicas: 5
    minReplicas: 1
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 50
  resources:
    limits:
      cpu: 1
      memory: 3Gi
    requests:
      cpu: 1
      memory: 3Gi
configs:
  params:
    # Allows insecure connections to the Argo CD API server.
    server.insecure: "true"
    # https://aws.amazon.com/it/blogs/opensource/argo-cd-application-controller-scalability-testing-on-amazon-eks/
    # status/operations processors:
    # Argo CD utilizes two queues to perform reconciliation (status) and application syncing (operations).
    # The number of processors to service those queues are controlled with the status and operations processor settings.
    # When the application reconciliation or sync is too slow, you need to increase the status and operation queue processors
    # status processors, as the name suggests is responsible to get the app info (Synced, Health, OutOfSync, Degraded, Unknown, Suspended, Missing, Error etc)

    # Description: Sets the number of processors for managing sync operations.
    # Default Value: 10
    # Like status processors, this boosts parallelism at the cost of additional resource usage.
    controller.operation.processors: "50"

    # Description: Sets the number of processors for status reconciliation tasks.
    # Default Value: 20
    # Increasing processors improves parallelism but demands more CPU and memory resources.
    controller.status.processors: "50"

    # Whenever Application Controller shows Context deadline exceeded (this will happend for large manifests), 
    # it was unable to fetch the manifests from repo server within the mentioned controller.repo.server.timeout.secods
    # Default Value: 60s
    controller.repo.server.timeout.seconds: "180"

    # By default there is not limit for parallel manifest generation.
    # Repo server tries to generate all the manifests at a time. Which causes OMM killed issues and manifest generation will to too slow
    # Description: Limits concurrent repository server operations.
    # Default Value: 0 — means infinity
    reposerver.parallelism.limit: "30"

    # Description: Specifies the timeout for repository server operation
    # Default Value: 60
    # Increasing this timeout helps with syncing larger repositories but may delay detecting actual failures.
    server.repo.server.timeout.seconds: "180"

    # Description: Limits concurrent kubectl operations.
    # Default Value: 20
    # Increasing this helps manage larger deployments but could overwhelm cluster resources.
    controller.kubectl.parallelism.limit: "100"

    controller.diff.server.side: "false"
    
  cm:
    timeout.hard.reconciliation: 300s
    timeout.reconciliation: 120s
    timeout.reconciliation.jitter: 60s
    resource.exclusions: |
      - apiGroups:
          - coordination.k8s.io
        kinds:
          - Lease
        clusters:
          - "*"
      - apiGroups:
          - ""
        kinds:
          - Endpoints
          - Event
        clusters:
          - "*"

    resource.customizations.health.batch_CronJob: |
      -- Custom health-check for batch/v1 CronJob.
      -- DATA READS:
      --   - obj.spec.suspend: whether the CronJob is paused
      --   - obj.status.active: list of currently active Jobs (length indicates running executions)
      --   - obj.status.lastScheduleTime: last time the CronJob was scheduled
      --   - obj.status.lastSuccessfulTime: last time a scheduled Job completed successfully
      -- EFFECT IN THE UI:
      --   - Colors the CronJob node in Argo CD trees:
      --       Degraded (red)   -> suspended or last scheduled run not (yet) successful
      --       Progressing (yellow) -> job currently active or never ran yet
      --       Healthy (green)  -> last scheduled run completed successfully

      hs = { status = "Progressing", message = "" }

      -- Missing spec -> cannot evaluate, keep Progressing with hint
      -- UI: CronJob shows Progressing until spec/status is available
      if obj == nil or obj.spec == nil then
        hs.message = "no spec"
        return hs
      end

      -- Suspended CronJob -> Degraded
      -- UI: immediately red with an explicit message
      if obj.spec.suspend == true then
        hs.status  = "Degraded"
        hs.message = "CronJob is suspended"
        return hs
      end

      -- Snapshot status fields
      local status = obj.status or {}
      local active = (status.active and #status.active) or 0
      local lst    = status.lastScheduleTime
      local lst_ok = status.lastSuccessfulTime

      -- Active job(s) -> Progressing
      -- UI: yellow showing how many jobs are currently running
      if active > 0 then
        hs.status  = "Progressing"
        hs.message = "Active Jobs: " .. tostring(active)
        return hs
      end

      -- History-based health: if we have a last schedule time, verify success after that schedule.
      if lst ~= nil then

        -- No recorded success yet after last schedule -> Degraded
        -- UI: red indicating the most recent run hasn't succeeded
        if lst_ok == nil then
          hs.status  = "Degraded"
          hs.message = "Last run has no successful completion"
          return hs
        end

        -- lastSuccessfulTime < lastScheduleTime -> last scheduled run not successful yet -> Degraded
        -- (RFC3339 strings compare lexicographically for ordering)
        if lst_ok < lst then
          hs.status  = "Degraded"
          hs.message = "Last scheduled run not successful yet"
          return hs
        end
        
        -- 5) Last run succeeded -> Healthy
        -- green with confirmation message
        hs.status  = "Healthy"
        hs.message = "Last run successful"
        return hs
      end

      -- Never ran yet -> Progressing
      -- UI: yellow until the first successful execution occurs
      hs.status  = "Progressing"
      hs.message = "No runs yet"

      return hs

    resource.customizations.health.apps_Deployment: |
      -- Custom health-check for apps/v1 Deployment.
      -- DATA READS:
      --   - obj.status.conditions: controller conditions (Progressing/Available with reason/status)
      --   - obj.spec.replicas: desired replicas (defaults to 1)
      --   - obj.status.updatedReplicas / readyReplicas / availableReplicas
      --   - obj.status.observedGeneration vs obj.metadata.generation
      -- EFFECT IN THE UI:
      --   - Colors the Deployment node in Argo CD trees: Degraded (red) for hard rollout failures,
      --     Progressing (yellow) while rolling out, Healthy (green) when all replicas are updated & available.

      hs = { status = "Progressing", message = "" }

      -- No status yet -> still progressing
      -- UI: Deployment shows Progressing until controller fills status
      if obj.status == nil then
        hs.message = "no status yet"; 
        return hs
      end

      -- Hard failure: ProgressDeadlineExceeded
      -- UI: immediate Degraded with reason when rollout exceeded progress deadline
      if obj.status.conditions then
        for _,c in ipairs(obj.status.conditions) do
          if c.type=="Progressing" and c.status=="False" then
            if c.reason=="ProgressDeadlineExceeded" then
              hs.message="Progress deadline exceeded"; 
              hs.status="Degraded"; 
              
              return hs
            end          
          end
          -- (Optional debug hooks kept commented)
          -- hs.message = c.reason .. " " .. c.status .. " " .. c.type
          -- hs.status  = "Degraded"
          -- return hs
          -- if c.type == "Available" and c.status == "False" then
          --   hs.status  = "Degraded"
          --   hs.message = c.message or "Unavailable"
          --   return hs
          -- end
        end
      end

      -- Snapshot rollout counters
      local desired = (obj.spec and obj.spec.replicas) or 1
      local updated = (obj.status and obj.status.updatedReplicas) or 0
      local ready   = (obj.status and obj.status.readyReplicas) or 0
      local available  = (obj.status and obj.status.availableReplicas) or 0
      local observed   = (obj.status and obj.status.observedGeneration) or 0
      local generation = (obj.metadata and obj.metadata.generation) or 0
      
      -- Controller hasn’t observed the new spec yet -> Progressing
      -- UI: keeps yellow while waiting for the new generation to be picked up
      if observed < generation then
        hs.message="waiting for controller to observe new generation"; 
        return hs
      end

      -- Heuristic for pod crash/backoff: rollout applied (all updated) but zero ready/available pods.
      -- Degrade immediately; if Available=False is present, surface its reason.
      -- UI: Deployment turns red to reflect likely pod crash/backoff      
      if updated >= desired and available == 0 and ready == 0 then
        if obj.status.conditions then
          for _, c in ipairs(obj.status.conditions) do
            if c.type == "Available" and c.status == "False" then
              hs.status  = "Degraded"
              hs.message = c.reason or "No pods available (likely crash/backoff)"
              return hs
            end
          end
        end

        hs.status  = "Degraded"
        hs.message = "No pods available after rollout (likely crash/backoff)"
        return hs
      end
      
      -- Still updating pods -> Progressing
      -- UI: yellow with a counter of updated vs desired
      if updated < desired then
        hs.message = "updatedReplicas " .. tostring(updated) .. "/" .. tostring(desired);
        return hs
      end

      -- All updated but not yet available -> Progressing
      -- UI: yellow with a counter of available vs desired (readiness/availability catching up)
      if available < desired then
        hs.message = "availableReplicas " .. tostring(available) .. "/" .. tostring(desired);
        return hs
      end

      -- Rollout complete and healthy
      -- UI: Deployment shows Healthy (green)
      hs.status  = "Healthy"; 
      hs.message = "All replicas updated and available";

      return hs
      
    resource.customizations.health.v1_Pod: |
      -- Custom health-check for core/v1 Pod.
      -- DATA READS:
      --   - obj.status.phase: high-level Pod phase (Pending/Running/Succeeded/Failed/Unknown)
      --   - obj.status.initContainerStatuses / containerStatuses:
      --       * .state.waiting.reason     -> e.g., ImagePullBackOff, CrashLoopBackOff, ContainerCreating
      --       * .state.terminated.reason  -> e.g., OOMKilled, Error
      --       * .restartCount             -> used to threshold CrashLoopBackOff
      --   - obj.status.conditions: looks for Ready=True when phase=Running
      -- EFFECT IN THE UI:
      --   - Colors Pod nodes: Degraded (red) for hard failures, Progressing (yellow) for warm-up/backoff,
      --     Healthy (green) when Running and Ready=True. Otherwise remains Progressing by default.

      hs = { status = "Progressing", message = "" }
      if not obj.status then 
        -- UI: Pod shows Progressing until status is available
        return hs 
      end

      -- Immediate failure via phase
      -- UI: Pod turns Degraded with reason when phase=Failed
      local phase = obj.status.phase or "Unknown"
      if phase == "Failed" then
        hs.status  = "Degraded"
        hs.message = obj.status.reason or "Pod Failed"
        return hs
      end

      -- Inspect init/app containers for actionable states
      -- Returns true if a decisive status was found (and hs was set).
      local function scan(statuses)
        for _, cs in ipairs(statuses or {}) do
          local waiting = cs.state and cs.state.waiting
          local term    = cs.state and cs.state.terminated
          local rc      = cs.restartCount or 0

          if waiting and waiting.reason then
            local r = waiting.reason
            
            -- Pull/config errors -> immediate Degraded
            -- UI: Pod goes red with the specific reason
            if r == "ImagePullBackOff" or r == "ErrImagePull" or r == "CreateContainerConfigError" then
              hs.status  = "Degraded"
              hs.message = r
              return true
            end
            
            -- CrashLoopBackOff -> tolerate brief flaps, degrade after threshold
            -- UI: <=2 restarts -> Progressing; >=3 -> Degraded with restart count
            if r == "CrashLoopBackOff" then
              if rc >= 3 then
                hs.status  = "Degraded"; hs.message = r .. " (restarts=" .. tostring(rc) .. ")"
              else
                hs.status  = "Progressing"; hs.message = r .. " (restarts=" .. tostring(rc) .. ")"
              end
              return true
            end
            
            -- Normal warm-up states -> Progressing
            -- UI: Pod shows Progressing while containers are being created/initialized
            if r == "ContainerCreating" or r == "PodInitializing" then
              hs.status  = "Progressing"; hs.message = r
              return true
            end
          end

          -- Termination reasons that imply failure
          -- UI: Pod becomes Degraded with termination reason
          if term and term.reason then
            if term.reason == "OOMKilled" or term.reason == "Error" then
              hs.status  = "Degraded"; hs.message = term.reason
              return true
            end
          end
        end
        return false
      end

      -- Check init containers first, then main containers
      if scan(obj.status.initContainerStatuses) then return hs end
      if scan(obj.status.containerStatuses)    then return hs end

      -- Healthy only when Running AND Ready=True
      -- UI: Pod turns Healthy (green) when Ready condition is True
      if phase == "Running" and obj.status.conditions then
        for _, c in ipairs(obj.status.conditions) do
          if c.type == "Ready" and c.status == "True" then
            hs.status  = "Healthy"; hs.message = "Ready"; return hs
          end
        end
      end

      -- Default: still Progressing (covers Pending/Unknown/Running but not Ready)
      -- UI: Pod remains yellow awaiting readiness or further signals
      return hs

    resource.customizations.health.argoproj.io_Application: |
      -- Custom health-check for argoproj.io/Application.
      -- DATA READS:
      --   - obj.status.conditions: controller-level conditions (e.g., ComparisonError, SyncError)
      --   - obj.status.operationState.phase/message: current sync operation phase (Running/Failed/Error)
      --   - obj.status.sync.status: app-level sync status (Synced/OutOfSync/Unknown/...)
      --   - obj.status.health.status/message: aggregated health of child resources
      -- EFFECT IN THE UI:
      --   - Sets the Application node’s "Health" in trees/tiles where Application appears as a child.
      --   - Drives color: Degraded (red) / Progressing (yellow) / Healthy (green) / Unknown (gray).
      -- NOTE: On the main Applications list, the tile’s Health is still computed by the controller’s
      --       aggregation; this override is applied when the Application is rendered as a resource node.

      hs = { status = "Progressing", message = "" }

      -- No status yet -> Progressing
      -- UI: show "Progressing" while the controller hasn’t populated status.
      if obj.status == nil then
        hs.status = "Progressing"
        hs.message = "Application has no status yet"
        return hs
      end

      -- Well known error conditions from controller (reconciliation/diff/invalid spec) -> Degraded
      -- If any of these conditions is True, we degrade immediately and surface the message.
      -- UI: Application node turns "Degraded" with the specific error message.
      local function has_error_condition()
        if not obj.status.conditions then 
          return nil 
        end

        for _, c in ipairs(obj.status.conditions) do
          if c.status == "True" and (
              c.type == "ComparisonError" or
              c.type == "InvalidSpecError" or
              c.type == "SyncError" or
              c.type == "OperationError"
            ) then
            return c.message or c.type
          end
        end

        return nil
      end

      local cond_err = has_error_condition()
      if cond_err then
        hs.status  = "Degraded"
        hs.message = cond_err
        return hs
      end

      -- Operation phase handling:
      --  - Error/Failed -> Degraded with message
      --  - Running      -> Progressing
      -- UI: reflects live sync operation state on the node.
      if obj.status.operationState ~= nil then
        local phase = obj.status.operationState.phase
        -- Failed operation / specific error
        if phase == "Error" or phase == "Failed" then
          hs.status  = "Degraded"
          hs.message = (obj.status.operationState.message or "Sync operation failed")
          return hs
        -- During an active sync, show Progressing
        elseif phase == "Running" then
          hs.status  = "Progressing"
          hs.message = "Sync in progress"
          return hs
        end
      end

      -- Sync-to-health mapping for visibility:
      --  - OutOfSync -> mark as Missing (treated as not in sync)
      --  - Any non-Synced (including Unknown) -> Degraded
      -- UI: makes the Application node show red/yellow when it’s not synced.
      local sync = (obj.status.sync and obj.status.sync.status) or "Unknown"

      -- Handle OutOfSync and !sync statuses
      if sync == "OutOfSync" then
        hs.status = "Missing"
        hs.message = "Application is " .. sync
        return hs
      end

      if sync ~= "Synced" then
        hs.status = "Degraded"
        hs.message = "Application is " .. sync
        return hs
      end

      -- Synced: defer to aggregated child health
      -- If aggregated health is present, mirror it; otherwise mark Unknown with a hint.
      -- UI: node shows Healthy/Progressing/Degraded based on children.
      if obj.status.health and obj.status.health.status then
        hs.status  = obj.status.health.status
        hs.message = obj.status.health.message or "No health reported"
      else
        hs.status  = "Unknown"
        hs.message = "No health reported"
      end

      return hs
    
    resource.customizations.health.argoproj.io_ApplicationSet: |
      -- Custom health-check for ApplicationSet.
      -- DATA READS:
      --   - obj.status.conditions: conditions from the ApplicationSet controller (e.g., ErrorOccurred)
      --   - obj.status.resources: list of generated Applications, with:
      --       res.name            -> child Application name
      --       res.health.status   -> aggregated health of the Application (Healthy/Progressing/Degraded/Missing/...)
      --       res.status          -> sync status of the Application (Synced/OutOfSync/Unknown/...)
      -- EFFECT IN THE UI:
      --   - Sets the "Health" of the ApplicationSet node in Argo CD trees/tiles.
      --   - Shows Degraded/Progressing/Healthy based on child state and controller conditions.

      hs = {}

      -- If status is not yet populated by the controller, treat as progressing.
      -- UI: ApplicationSet shows "Progressing" with an explanatory message.
      if obj.status == nil then
        hs.status = "Progressing"
        hs.message = "Waiting for ApplicationSet controller to populate status"
        return hs
      end

      -- Handle known controller-level errors (e.g., template/generation failed).
      -- If ErrorOccurred=True, degrade immediately.
      -- UI: ApplicationSet shows "Degraded" with the controller error message.
      if obj.status.conditions ~= nil then
        for _, c in ipairs(obj.status.conditions) do
          if c.type == "ErrorOccurred" and c.status == "True" then
            hs.status = "Degraded"
            hs.message = c.message or "ApplicationSet controller reported an error"
            return hs
          end
        end
      end

      -- If no Applications have been generated yet, we’re still progressing.
      -- UI: "Progressing" with explanation.
      local hasResources = obj.status.resources ~= nil and #obj.status.resources > 0
      if not hasResources then
        hs.status = "Progressing"
        hs.message = "No generated Applications yet"
        return hs
      end

      -- Classify generated Applications:
      --  - unhealthy: children with health Degraded (red)
      --  - progressing: children with health Progressing (yellow)
      --  - outofsync: children with health Missing (drift) or sync != Synced
      -- UI: these lists drive the final ApplicationSet health below.
      local unhealthy = {}
      local progressing = {}
      local outofsync = {}

      for _, res in ipairs(obj.status.resources) do
        local h = (res.health and res.health.status) or "Unknown"
        local s = res.status or "Unknown"
        if h == "Degraded" then
          table.insert(unhealthy, res.name) -- child is red
        elseif h == "Missing" then
          table.insert(outofsync, res.name) -- child is out of sync (missing)
        elseif h == "Progressing" then
          table.insert(progressing, res.name) -- child is in progress
        elseif s ~= "Synced" then
          table.insert(outofsync, res.name) -- any other non-synced case
        end
      end

      -- Final health decision for the ApplicationSet based on children.
      -- Priority: Degraded > Progressing (OutOfSync) > Progressing (in progress) > Healthy
      -- UI:
      --    - If any child is Degraded -> ApplicationSet "Degraded" (red).
      --    - Else, if any child is OutOfSync/Missing -> "Progressing" (yellow) with list.
      --    - Else, if any child is Progressing -> "Progressing" (yellow).
      --    - If all are Healthy & Synced -> "Healthy" (green).
      if #unhealthy > 0 then
        hs.status = "Degraded"
        hs.message = "Unhealthy Applications: " .. table.concat(unhealthy, ", ")
        return hs
      elseif #outofsync > 0 then
        hs.status = "Progressing"
        hs.message = "OutOfSync Applications: " .. table.concat(outofsync, ", ")
        return hs
      elseif #progressing > 0 then
        hs.status = "Progressing"
        hs.message = "Applications still progressing: " .. table.concat(progressing, ", ")
        return hs
      else
        hs.status = "Healthy"
        hs.message = "All generated Applications are Healthy and Synced"
        return hs
      end