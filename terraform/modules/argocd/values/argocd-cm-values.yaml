global:
  logging:
    level: info
    format: text
  env:
      # Description: Limits the number of Kubernetes API requests per second.
      # Default Value: 50
      # Higher values reduce bottlenecks in syncing large applications but can strain the Kubernetes API server. Monitor server metrics to ensure it can handle the increased load.
    - name: ARGOCD_K8S_CLIENT_QPS
      value: "100"
    
      # Description: Defines the maximum burst of requests sent to the Kubernetes API.
      # Default Value: 100
      # Increasing the burst limit improves sync speeds during spikes but could lead to API server throttling if it exceeds capacity.
    - name: ARGOCD_K8S_CLIENT_BURST
      value: "200"
redis-ha:
  enabled: false
redis:
  metrics:
    enabled: false
    service:
      annotations: {}
      clusterIP: None
      labels: {}
      portName: http-metrics
      servicePort: 9121
      type: ClusterIP
    serviceMonitor:
      additionalLabels: {}
      annotations: {}
      enabled: false
      honorLabels: false
      interval: 30s
      metricRelabelings: []
      namespace: ""
      relabelings: []
      scheme: ""
      selector: {}
      tlsConfig: {}  
  resources:
    limits:
      cpu: 200m
      memory: 256Mi
    requests:
      cpu: 10m
      memory: 64Mi
applicationSet:
  metrics:
    enabled: true
    service:
      annotations: {}
      clusterIP: ""
      labels: {}
      portName: http-metrics
      servicePort: 8080
      type: ClusterIP
    serviceMonitor:
      additionalLabels: {}
      annotations: {}
      enabled: true
      honorLabels: false
      interval: 30s
      metricRelabelings: []
      namespace: ""
      relabelings: []
      scheme: ""
      scrapeTimeout: ""
      selector: {}
      tlsConfig: {}
  name: applicationset-controller
  replicas: 1
  resources:
    limits:
      cpu: 150m
      memory: 256Mi
    requests:
      cpu: 80m
      memory: 64Mi
  service:
    annotations: {}
    labels: {}
    port: 7000
    portName: http-webhook
    type: ClusterIP
controller:
  metrics:
    enabled: true
  replicas: 1
  resources:
    limits:
      cpu: 2
      memory: 4Gi
    requests:
      cpu: 2
      memory: 4Gi
  revisionHistoryLimit: 5
repoServer:
  autoscaling:
    enabled: false
    maxReplicas: 5
    minReplicas: 1
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 50
  volumes:
    - name: plugin-home
      emptyDir: {}
    - name: cmp-tmp
      emptyDir: {}
  extraArgs: []
  extraContainers:
    - name: ${microservices_plugin_name}
      image: ${microservices_plugin_image}
      imagePullPolicy: IfNotPresent
      command:
        - /var/run/argocd/argocd-cmp-server
      securityContext:
        runAsNonRoot: true
        runAsUser: 999
      volumeMounts:
        - mountPath: /var/run/argocd
          name: var-files
        - mountPath: /home/argocd/cmp-server/plugins
          name: plugins
        - mountPath: /home/argocd/plugin
          name: plugin-home
        - mountPath: /tmp
          name: cmp-tmp
      env:
          # Repo server uses “exec” to run helm, kustomize, etc… for a large helm chart manifest generation may take time. 
          # Description: Defines the maximum time (in seconds) a command can execute before timing out.
          # Default Value: 90s
        - name: ARGOCD_EXEC_TIMEOUT
          value: "5m"
          # Repo server uses “git ls-remote” to resolve commit SHA hash. Default value is 1, means if the first attempt fails it stop to check agian. 
          # Description: Specifies the number of retry attempts for Git operations.
          # Default Value: 1
        - name: ARGOCD_GIT_ATTEMPTS_COUNT
          value: "3"
          # Starting v3.0, Argo CD performs a parallel generation of Helm manifests by default.
        - name: ARGOCD_EXEC_FATAL_TIMEOUT
          value: "5m"
        - name: XDG_CONFIG_HOME
          value: "/home/argocd/plugin/.config"
        - name: HELM_CONFIG_HOME
          value: "/home/argocd/plugin/.config/helm"
        - name: HELM_DATA_HOME
          value:  "/home/argocd/plugin/.local/share/helm"
        - name: HELM_CACHE_HOME
          value: "/home/argocd/plugin/.cache/helm"
    - name: ${cronjobs_plugin_name}
      image: ${cronjobs_plugin_image}
      imagePullPolicy: IfNotPresent
      command:
        - /var/run/argocd/argocd-cmp-server
      securityContext:
        runAsNonRoot: true
        runAsUser: 999
      volumeMounts:
        - mountPath: /var/run/argocd
          name: var-files
        - mountPath: /home/argocd/cmp-server/plugins
          name: plugins
        - mountPath: /home/argocd/plugin
          name: plugin-home
        - mountPath: /tmp
          name: cmp-tmp
      env:
          # Repo server uses “exec” to run helm, kustomize, etc… for a large helm chart manifest generation may take time. 
          # Description: Defines the maximum time (in seconds) a command can execute before timing out.
          # Default Value: 90s
        - name: ARGOCD_EXEC_TIMEOUT
          value: "5m"
          # Repo server uses “git ls-remote” to resolve commit SHA hash. Default value is 1, means if the first attempt fails it stop to check agian. 
          # Description: Specifies the number of retry attempts for Git operations.
          # Default Value: 1
        - name: ARGOCD_GIT_ATTEMPTS_COUNT
          value: "3"
        - name: ARGOCD_HELM_ALLOW_CONCURRENCY
          value: "true"
        - name: ARGOCD_EXEC_FATAL_TIMEOUT
          value: "5m"
        - name: XDG_CONFIG_HOME
          value: "/home/argocd/plugin/.config"
        - name: HELM_CONFIG_HOME
          value: "/home/argocd/plugin/.config/helm"
        - name: HELM_DATA_HOME
          value:  "/home/argocd/plugin/.local/share/helm"
        - name: HELM_CACHE_HOME
          value: "/home/argocd/plugin/.cache/helm"
  metrics:
    enabled: false
    service:
      annotations: {}
      clusterIP: ""
      labels: {}
      portName: http-metrics
      servicePort: 8084
      type: ClusterIP
    serviceMonitor:
      additionalLabels: {}
      annotations: {}
      enabled: true
      honorLabels: false
      interval: 30s
      metricRelabelings: []
      namespace: ""
      relabelings: []
      scheme: ""
      scrapeTimeout: ""
      selector: {}
      tlsConfig: {}
  replicas: 2
  resources:
    limits:
      cpu: 1
      memory: 4Gi
    requests:
      cpu: 1
      memory: 4Gi
server:
  replicas: 2
  autoscaling:
    enabled: false
    maxReplicas: 5
    minReplicas: 1
    targetCPUUtilizationPercentage: 50
    targetMemoryUtilizationPercentage: 50
  resources:
    limits:
      cpu: 1
      memory: 3Gi
    requests:
      cpu: 1
      memory: 3Gi
configs:
  params:
    # Allows insecure connections to the Argo CD API server.
    server.insecure: "true"
    # https://aws.amazon.com/it/blogs/opensource/argo-cd-application-controller-scalability-testing-on-amazon-eks/
    # status/operations processors:
    # Argo CD utilizes two queues to perform reconciliation (status) and application syncing (operations).
    # The number of processors to service those queues are controlled with the status and operations processor settings.
    # When the application reconciliation or sync is too slow, you need to increase the status and operation queue processors
    # status processors, as the name suggests is responsible to get the app info (Synced, Health, OutOfSync, Degraded, Unknown, Suspended, Missing, Error etc)

    # Description: Sets the number of processors for managing sync operations.
    # Default Value: 10
    # Like status processors, this boosts parallelism at the cost of additional resource usage.
    controller.operation.processors: "50"

    # Description: Sets the number of processors for status reconciliation tasks.
    # Default Value: 20
    # Increasing processors improves parallelism but demands more CPU and memory resources.
    controller.status.processors: "50"

    # Whenever Application Controller shows Context deadline exceeded (this will happend for large manifests), 
    # it was unable to fetch the manifests from repo server within the mentioned controller.repo.server.timeout.secods
    # Default Value: 60s
    controller.repo.server.timeout.seconds: "180"

    # By default there is not limit for parallel manifest generation.
    # Repo server tries to generate all the manifests at a time. Which causes OMM killed issues and manifest generation will to too slow
    # Description: Limits concurrent repository server operations.
    # Default Value: 0 — means infinity
    reposerver.parallelism.limit: "30"

    # Description: Specifies the timeout for repository server operation
    # Default Value: 60
    # Increasing this timeout helps with syncing larger repositories but may delay detecting actual failures.
    server.repo.server.timeout.seconds: "180"

    # Description: Limits concurrent kubectl operations.
    # Default Value: 20
    # Increasing this helps manage larger deployments but could overwhelm cluster resources.
    controller.kubectl.parallelism.limit: "100"

    controller.diff.server.side: "false"
    
  cm:
    timeout.hard.reconciliation: 300s
    timeout.reconciliation: 120s
    timeout.reconciliation.jitter: 60s
    resource.exclusions: |
      - apiGroups:
          - coordination.k8s.io
        kinds:
          - Lease
        clusters:
          - "*"
      - apiGroups:
          - ""
        kinds:
          - Endpoints
          - Event
        clusters:
          - "*"

    resource.customizations.health.batch_CronJob: |
      hs = { status = "Progressing", message = "" }

      if obj == nil or obj.spec == nil then
        hs.message = "no spec"
        return hs
      end

      -- CronJob suspended
      if obj.spec.suspend == true then
        hs.status  = "Degraded"
        hs.message = "CronJob is suspended"
        return hs
      end

      local status = obj.status or {}
      local active = (status.active and #status.active) or 0
      local lst    = status.lastScheduleTime
      local lst_ok = status.lastSuccessfulTime

      -- There's an active Job
      if active > 0 then
        hs.status  = "Progressing"
        hs.message = "Active Jobs: " .. tostring(active)
        return hs
      end

      -- If the Job has been scheduled but not yet run successfully after the last schedule -> probably a failure
      if lst ~= nil then
        if lst_ok == nil then
          hs.status  = "Degraded"
          hs.message = "Last run has no successful completion"
          return hs
        end
        -- If last successful time is before last scheduled time -> probably a failure
        if lst_ok < lst then
          hs.status  = "Degraded"
          hs.message = "Last scheduled run not successful yet"
          return hs
        end
        
        -- Last run was successful
        hs.status  = "Healthy"
        hs.message = "Last run successful"
        return hs
      end

      -- Not yet run
      hs.status  = "Progressing"
      hs.message = "No runs yet"
      return hs
      
    resource.customizations.health.apps_Deployment: |
      hs = { status = "Progressing", message = "" }

      if obj.status == nil then
        hs.message = "no status yet"; 
        return hs
      end

      -- Degrade on ProgressDeadlineExceeded
      if obj.status.conditions then
        for _,c in ipairs(obj.status.conditions) do
          if c.type=="Progressing" and c.status=="False" then
            if c.reason=="ProgressDeadlineExceeded" then
              hs.message="Progress deadline exceeded"; 
              hs.status="Degraded"; 
              
              return hs
            end          
          end
          -- hs.message=c.reason .. " " .. c.status .. " " .. c.type; 
          -- hs.status="Degraded"; 
            
          -- return hs 
          --if c.type=="Available" and c.status=="False" then
          --  hs.status="Degraded"; 
          --  hs.message=c.message or "Unavailable";
            
          --  return hs
          --end
        end
      end

      local desired = (obj.spec and obj.spec.replicas) or 1
      local updated = (obj.status and obj.status.updatedReplicas) or 0
      local available = (obj.status and obj.status.availableReplicas) or 0
      local observed = (obj.status and obj.status.observedGeneration) or 0
      local generation = (obj.metadata and obj.metadata.generation) or 0

      if observed < generation then
        hs.message="waiting for controller to observe new generation"; 
        return hs
      end

      if updated < desired then
        hs.message = "updatedReplicas " .. tostring(updated) .. "/" .. tostring(desired);
        return hs
      end

      if available < desired then
        hs.message = "availableReplicas " .. tostring(available) .. "/" .. tostring(desired);
        return hs
      end

      hs.status="Healthy"; 
      hs.message="All replicas updated and available"; 
      return hs
      
    resource.customizations.health.v1_Pod: |
      hs = { status = "Progressing", message = "" }
      if not obj.status then return hs end

      local phase = obj.status.phase or "Unknown"
      if phase == "Failed" then
        hs.status  = "Degraded"
        hs.message = obj.status.reason or "Pod Failed"
        return hs
      end

      -- examine both initContainers and app containers
      local function scan(statuses)
        for _, cs in ipairs(statuses or {}) do
          local waiting = cs.state and cs.state.waiting
          local term    = cs.state and cs.state.terminated
          local rc      = cs.restartCount or 0

          if waiting and waiting.reason then
            local r = waiting.reason
            -- immediate degrade on pull/config errors
            if r == "ImagePullBackOff" or r == "ErrImagePull" or r == "CreateContainerConfigError" then
              hs.status  = "Degraded"
              hs.message = r
              return true
            end
            -- CrashLoopBackOff: patient until few restarts, then degrade
            if r == "CrashLoopBackOff" then
              if rc >= 3 then
                hs.status  = "Degraded"; hs.message = r .. " (restarts=" .. tostring(rc) .. ")"
              else
                hs.status  = "Progressing"; hs.message = r .. " (restarts=" .. tostring(rc) .. ")"
              end
              return true
            end
            -- normal warm-up states
            if r == "ContainerCreating" or r == "PodInitializing" then
              hs.status  = "Progressing"; hs.message = r
              return true
            end
          end

          if term and term.reason then
            if term.reason == "OOMKilled" or term.reason == "Error" then
              hs.status  = "Degraded"; hs.message = term.reason
              return true
            end
          end
        end
        return false
      end

      if scan(obj.status.initContainerStatuses) then return hs end
      if scan(obj.status.containerStatuses)    then return hs end

      -- Healthy only when Ready=True
      if phase == "Running" and obj.status.conditions then
        for _, c in ipairs(obj.status.conditions) do
          if c.type == "Ready" and c.status == "True" then
            hs.status  = "Healthy"; hs.message = "Ready"; return hs
          end
        end
      end

      return hs

    resource.customizations.health.argoproj.io_Application: |
      hs = {}
      hs.status = "Progressing"
      hs.message = ""

      if obj.status == nil then
        hs.status = "Progressing"
        hs.message = "Application has no status yet"
        return hs
      end

      -- During an active sync, show Progressing
      if obj.status.operationState ~= nil and obj.status.operationState.phase == "Running" then
        hs.status = "Progressing"
        hs.message = "Sync in progress"
        return hs
      end

      local sync = (obj.status.sync and obj.status.sync.status) or "Unknown"

      -- Handle OutOfSync and !sync statuses
      if sync == "OutOfSync" then
        hs.status = "Missing"
        hs.message = "Application is " .. sync
        return hs
      end

      if sync ~= "Synced" then
        hs.status = "Degraded"
        hs.message = "Application is " .. sync
        return hs
      end

      -- When synced, defer to the app aggregated health
      if obj.status.health and obj.status.health.status then
        hs.status  = obj.status.health.status
        hs.message = obj.status.health.message or "3"
      else
        hs.status  = "Unknown"
        hs.message = "No health reported"
      end

      return hs
    
    resource.customizations.health.argoproj.io_ApplicationSet: |
      hs = {}

      if obj.status == nil then
        hs.status = "Progressing"
        hs.message = "Waiting for ApplicationSet controller to populate status"
        return hs
      end

      -- Handle known error condition
      if obj.status.conditions ~= nil then
        for _, c in ipairs(obj.status.conditions) do
          if c.type == "ErrorOccurred" and c.status == "True" then
            hs.status = "Degraded"
            hs.message = c.message or "ApplicationSet controller reported an error"
            return hs
          end
        end
      end

      local hasResources = obj.status.resources ~= nil and #obj.status.resources > 0
      if not hasResources then
        hs.status = "Progressing"
        hs.message = "No generated Applications yet"
        return hs
      end

      local unhealthy = {}
      local progressing = {}
      local outofsync = {}

      for _, res in ipairs(obj.status.resources) do
        local h = (res.health and res.health.status) or "Unknown"
        local s = res.status or "Unknown"
        if h == "Degraded" then
        table.insert(unhealthy, res.name)
        elseif h == "Missing" then
          table.insert(outofsync, res.name)
        elseif h == "Progressing" then
          table.insert(progressing, res.name)
        elseif s ~= "Synced" then
          table.insert(outofsync, res.name)
        end
      end

      if #unhealthy > 0 then
        hs.status = "Degraded"
        hs.message = "Unhealthy Applications: " .. table.concat(unhealthy, ", ")
        return hs
      elseif #outofsync > 0 then
        hs.status = "Progressing"
        hs.message = "OutOfSync Applications: " .. table.concat(outofsync, ", ")
        return hs
      elseif #progressing > 0 then
        hs.status = "Progressing"
        hs.message = "Applications still progressing: " .. table.concat(progressing, ", ")
        return hs
      else
        hs.status = "Healthy"
        hs.message = "All generated Applications are Healthy and Synced"
        return hs
      end